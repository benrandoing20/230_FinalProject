{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ffede2-560b-4298-accc-f1009a262db4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47612abd-067e-406c-b184-966cbdb0aba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/cs230_project_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ")\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import (\n",
    "    RandomResizedCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    ColorJitter,\n",
    "    RandomGrayscale,\n",
    "    RandomApply,\n",
    "    Compose,\n",
    "    GaussianBlur,\n",
    "    ToTensor,\n",
    ")\n",
    "import torchvision.models as models\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b731e28b-dc13-4acd-81c1-f12d5d66d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac45140-19c4-4e86-84ff-0b9232efd3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e3afe-e025-407d-9e61-00a4a02bb6b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get device type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8cc711-7911-4efb-a460-80f6b4889059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch-Version 1.12.1.post201\n",
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f'Torch-Version {torch.__version__}')\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'DEVICE: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d07fb45-b6a6-49d4-a91d-dd222e54350f",
   "metadata": {},
   "source": [
    "## Define transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c77f905-2cd5-4452-9dd0-7cc4364c3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_transform(output_shape, kernel_size, s=1.0):\n",
    "    \"\"\"\n",
    "    The color distortion transform.\n",
    "    \n",
    "    Args:\n",
    "        s: Strength parameter.\n",
    "    \n",
    "    Returns:\n",
    "        A color distortion transform.\n",
    "    \"\"\"\n",
    "    rnd_crop = RandomResizedCrop(output_shape)\n",
    "    rnd_flip = RandomHorizontalFlip(p=0.5)\n",
    "    \n",
    "    color_jitter = ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
    "    # rnd_color_jitter = RandomApply([color_jitter], p=0.8)\n",
    "    \n",
    "    rnd_gray = RandomGrayscale(p=0.2)\n",
    "    gaussian_blur = GaussianBlur(kernel_size=kernel_size)\n",
    "    rnd_gaussian_blur = RandomApply([gaussian_blur], p=0.5)\n",
    "    to_tensor = ToTensor()\n",
    "\n",
    "    image_transform = Compose([\n",
    "        to_tensor,\n",
    "        rnd_crop,\n",
    "        rnd_flip,\n",
    "        # rnd_color_jitter,\n",
    "        rnd_gray,\n",
    "        rnd_gaussian_blur,\n",
    "    ])\n",
    "    return image_transform\n",
    "\n",
    "\n",
    "class ContrastiveLearningViewGenerator(object):\n",
    "    \"\"\"Take two random crops of one image as the query and key.\"\"\"\n",
    "\n",
    "    def __init__(self, base_transform, n_views=2):\n",
    "        self.base_transform = base_transform\n",
    "        self.n_views = n_views\n",
    "\n",
    "    def __call__(self, x):\n",
    "        views = [self.base_transform(x) for i in range(self.n_views)]\n",
    "        return views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7572e9-abd2-4987-941d-521373ceaa27",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c27909be-24d8-4550-948a-2e0b628e13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Flowers Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, list_images, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            list_images (list): List of all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.list_images = list_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_name = self.list_images[idx]\n",
    "        image = io.imread(img_name)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43a1d6db-1c12-47e4-9b0b-9a8d3c5f4c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the images\n",
    "output_shape = [299, 299]\n",
    "kernel_size = [21,21] # 10% of the output_shape\n",
    "\n",
    "# The custom transform\n",
    "base_transforms = get_complete_transform(output_shape=output_shape, kernel_size=kernel_size, s=1.0)\n",
    "custom_transform = ContrastiveLearningViewGenerator(base_transform=base_transforms)\n",
    "\n",
    "train_ds = CustomDataset(\n",
    "    list_images=glob.glob(\"scenario_2_with_nst/train/*/*.jpg\"),\n",
    "    transform=custom_transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4ed74-820f-451e-9063-74af9fafc06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_examples = len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609257d-b8d3-4c76-9aa3-1bffb09b9887",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 90:10 train val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a14e85-5b16-41cd-ae3a-71bd9114140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_1, val_ds = data.random_split(train_ds, [int(total_train_examples*0.9), total_train_examples - int(total_train_examples*0.9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa3b0acb-fcd0-4ad3-9b8d-c24404f25816",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m         plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mi)\n\u001b[1;32m     10\u001b[0m         plt\u001b[38;5;241m.\u001b[39mimshow(view2\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m---> 12\u001b[0m \u001b[43mview_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [9], line 4\u001b[0m, in \u001b[0;36mview_data\u001b[0;34m(transform_ds, index)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mview_data\u001b[39m(transform_ds, index):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m         images \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39marray(images[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m         view1, view2 \u001b[38;5;241m=\u001b[39m images\n",
      "Cell \u001b[0;32mIn [6], line 21\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(idx):\n\u001b[1;32m     19\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 21\u001b[0m img_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     22\u001b[0m image \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mimread(img_name)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x2000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "def view_data(transform_ds, index):\n",
    "    for i in range(1, 4):\n",
    "        images = transform_ds[index]\n",
    "        print(np.array(images[0]).shape)\n",
    "        view1, view2 = images\n",
    "        plt.subplot(5,2,2*i-1)\n",
    "        plt.imshow(view1.permute(1,2,0))\n",
    "        plt.subplot(5,2,2*i)\n",
    "        plt.imshow(view2.permute(1,2,0))\n",
    "\n",
    "view_data(train_ds, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf5201-89cb-4948-a897-3c308f571889",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "690ff952-02a9-42bb-b50b-9ca665c87994",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Building the data loader\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=os.cpu_count(),\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e5bc0-b71e-468e-bd76-886d06c889e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SimCLR model with pretrained InceptionV3 model for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd247188-abe3-4943-a25f-953b55171e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2a62990-d5e1-4273-bae9-974315f50aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b5d4485-d957-4047-8ffa-7bef60697b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, linear_eval=False):\n",
    "        super().__init__()\n",
    "        self.linear_eval = linear_eval\n",
    "        inceptionv3 = models.inception_v3(pretrained=True)\n",
    "        # set_parameter_requires_grad(inceptionv3, True)\n",
    "        inceptionv3.aux_logits = False\n",
    "        inceptionv3.fc = Identity()\n",
    "        inceptionv3.AuxLogits.fc = Identity()\n",
    "        self.encoder = inceptionv3\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.linear_eval:\n",
    "            x = torch.cat(x, dim=0)\n",
    "\n",
    "        encoding = self.encoder(x)\n",
    "        projection = self.projection(encoding) \n",
    "        return projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b429753-c724-465f-a166-0a277061893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = torch.cat([torch.arange(BATCH_SIZE) for i in range(2)], dim=0)\n",
    "LABELS = (LABELS.unsqueeze(0) == LABELS.unsqueeze(1)).float() # Creates a one-hot with broadcasting\n",
    "LABELS = LABELS.to(DEVICE) #128,128\n",
    "\n",
    "def cont_loss(features, temp):\n",
    "    \"\"\"\n",
    "    The NTxent Loss.\n",
    "    \n",
    "    Args:\n",
    "        z1: The projection of the first branch\n",
    "        z2: The projeciton of the second branch\n",
    "    \n",
    "    Returns:\n",
    "        the NTxent loss\n",
    "    \"\"\"\n",
    "    similarity_matrix = torch.matmul(features, features.T) # 128, 128\n",
    "    # discard the main diagonal from both: labels and similarities matrix\n",
    "    mask = torch.eye(LABELS.shape[0], dtype=torch.bool).to(DEVICE)\n",
    "    # ~mask is the negative of the mask\n",
    "    # the view is required to bring the matrix back to shape\n",
    "    labels = LABELS[~mask].view(LABELS.shape[0], -1) # 128, 127\n",
    "    similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1) # 128, 127\n",
    "\n",
    "    # select and combine multiple positives\n",
    "    positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1) # 128, 1\n",
    "\n",
    "    # select only the negatives\n",
    "    negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1) # 128, 126\n",
    "\n",
    "    logits = torch.cat([positives, negatives], dim=1) # 128, 127\n",
    "    labels = torch.zeros(logits.shape[0], dtype=torch.long).to(DEVICE)\n",
    "\n",
    "    logits = logits / temp\n",
    "    return logits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ebd60dc-beaf-4a26-bca1-d51c916ab949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/cs230_project_env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/cs230_project_env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "simclr_model = SimCLR().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(simclr_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2818cc96-bd1b-4552-8f7d-b10a9495d5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 BATCH: 10 LOSS: 0.4300 \n",
      "EPOCH: 1 BATCH: 20 LOSS: 0.3634 \n",
      "EPOCH: 1 BATCH: 30 LOSS: 0.3056 \n",
      "EPOCH: 1 BATCH: 40 LOSS: 0.2955 \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "train_loss_list = []\n",
    "for epoch in range(EPOCHS):\n",
    "    t0 = time.time()\n",
    "    running_loss = 0.0\n",
    "    for i, views in enumerate(train_dl):\n",
    "        projections = simclr_model([view.to(DEVICE) for view in views])\n",
    "        logits, labels = cont_loss(projections, temp=2)\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print(f'EPOCH: {epoch+1} BATCH: {i+1} LOSS: {(running_loss/100):.4f} ')\n",
    "            train_loss_list.append(running_loss)\n",
    "            running_loss = 0.0\n",
    "    print(f'Time taken: {((time.time()-t0)/60):.3f} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69e5f068-4a40-4d27-beaa-cc0bf63bb3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2104"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "# del variables\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee1cce-860b-4a9c-b8bf-c1a10bea7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(simclr_model.state_dict(), \"simclr_weights_scenario_no_nst.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd4ca2e-6da9-45d2-bc2d-3757cf1d0932",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Downstream task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f51fa94d-7a06-42db-8841-b2396638d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Resize, \n",
    "    ToTensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "07919cf1-f79a-4460-8df5-f6056961c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = Resize(400)\n",
    "ccrop = CenterCrop(299)\n",
    "ttensor = ToTensor()\n",
    "\n",
    "custom_transform = Compose([\n",
    "    resize,\n",
    "    ccrop,\n",
    "    ttensor,\n",
    "])\n",
    "\n",
    "train_ds_downstream = ImageFolder(\n",
    "    root=\"scenario_1_no_nst/train\",\n",
    "    transform=custom_transform\n",
    ")\n",
    "\n",
    "nu_classes = len(train_ds_downstream.classes)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Building the data loader\n",
    "train_dl_downstream = torch.utils.data.DataLoader(\n",
    "    train_ds_downstream,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=os.cpu_count(),\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "15c768da-e3a5-4f13-8d52-39f2c67c1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class LinearEvaluation(nn.Module):\n",
    "    def __init__(self, model, nu_classes):\n",
    "        super().__init__()\n",
    "        simclr = model\n",
    "        simclr.linear_eval = True\n",
    "        simclr.projection = Identity()\n",
    "        self.simclr = simclr\n",
    "        for param in self.simclr.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.linear1 = nn.Linear(2048, 512)\n",
    "        self.linear2 = nn.Linear(512, nu_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoding = self.simclr(x)\n",
    "        encoding = self.linear1(encoding)\n",
    "        encoding = nn.Dropout(0.8)(encoding)\n",
    "        pred = self.linear2(encoding)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1405e623-bbd8-4495-9b1e-e7fe88209838",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = LinearEvaluation(simclr_model, nu_classes).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(eval_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13309c60-2e41-49de-afba-059f0b483e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 BATCH: 10 LOSS: 0.0462 \n",
      "EPOCH: 1 BATCH: 20 LOSS: 0.0353 \n",
      "EPOCH: 1 BATCH: 30 LOSS: 0.0302 \n",
      "EPOCH: 1 BATCH: 40 LOSS: 0.0340 \n",
      "EPOCH: 1 BATCH: 50 LOSS: 0.0328 \n",
      "EPOCH: 1 BATCH: 60 LOSS: 0.0318 \n",
      "EPOCH: 1 BATCH: 70 LOSS: 0.0329 \n",
      "EPOCH: 1 BATCH: 80 LOSS: 0.0335 \n",
      "EPOCH: 1 BATCH: 90 LOSS: 0.0325 \n",
      "EPOCH: 1 BATCH: 100 LOSS: 0.0309 \n",
      "EPOCH: 1 BATCH: 110 LOSS: 0.0333 \n",
      "EPOCH: 1 BATCH: 120 LOSS: 0.0348 \n",
      "EPOCH: 1 BATCH: 130 LOSS: 0.0299 \n",
      "EPOCH: 1 BATCH: 140 LOSS: 0.0312 \n",
      "EPOCH: 1 BATCH: 150 LOSS: 0.0332 \n",
      "EPOCH: 1 BATCH: 160 LOSS: 0.0327 \n",
      "EPOCH: 1 BATCH: 170 LOSS: 0.0344 \n",
      "EPOCH: 1 BATCH: 180 LOSS: 0.0313 \n",
      "EPOCH: 1 BATCH: 190 LOSS: 0.0299 \n",
      "EPOCH: 1 BATCH: 200 LOSS: 0.0322 \n",
      "EPOCH: 1 BATCH: 210 LOSS: 0.0342 \n",
      "EPOCH: 1 BATCH: 220 LOSS: 0.0318 \n",
      "EPOCH: 1 BATCH: 230 LOSS: 0.0311 \n",
      "EPOCH: 1 BATCH: 240 LOSS: 0.0300 \n",
      "EPOCH: 1 BATCH: 250 LOSS: 0.0324 \n",
      "EPOCH: 1 BATCH: 260 LOSS: 0.0323 \n",
      "EPOCH: 1 BATCH: 270 LOSS: 0.0302 \n",
      "EPOCH: 1 BATCH: 280 LOSS: 0.0300 \n",
      "EPOCH: 1 BATCH: 290 LOSS: 0.0309 \n",
      "Time taken: 1.102 mins\n",
      "EPOCH: 2 BATCH: 10 LOSS: 0.0322 \n",
      "EPOCH: 2 BATCH: 20 LOSS: 0.0305 \n",
      "EPOCH: 2 BATCH: 30 LOSS: 0.0327 \n",
      "EPOCH: 2 BATCH: 40 LOSS: 0.0317 \n",
      "EPOCH: 2 BATCH: 50 LOSS: 0.0297 \n",
      "EPOCH: 2 BATCH: 60 LOSS: 0.0311 \n",
      "EPOCH: 2 BATCH: 70 LOSS: 0.0308 \n",
      "EPOCH: 2 BATCH: 80 LOSS: 0.0306 \n",
      "EPOCH: 2 BATCH: 90 LOSS: 0.0309 \n",
      "EPOCH: 2 BATCH: 100 LOSS: 0.0292 \n",
      "EPOCH: 2 BATCH: 110 LOSS: 0.0297 \n",
      "EPOCH: 2 BATCH: 120 LOSS: 0.0307 \n",
      "EPOCH: 2 BATCH: 130 LOSS: 0.0336 \n",
      "EPOCH: 2 BATCH: 140 LOSS: 0.0322 \n",
      "EPOCH: 2 BATCH: 150 LOSS: 0.0333 \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "eval_model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    t0 = time.time()\n",
    "    running_loss = 0.0\n",
    "    for i, element in enumerate(train_dl_downstream):\n",
    "        image, label = element\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        pred = eval_model(image)\n",
    "        loss = criterion(pred, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print(f'EPOCH: {epoch+1} BATCH: {i+1} LOSS: {(running_loss/100):.4f} ')\n",
    "            running_loss = 0.0\n",
    "    print(f'Time taken: {((time.time()-t0)/60):.3f} mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06099f0a-4324-4a6d-a118-4631b202b4d8",
   "metadata": {},
   "source": [
    "## Test data performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea02c4d-5592-4074-b663-be988f12a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transform = Compose([\n",
    "    resize,\n",
    "    ccrop,\n",
    "    ttensor,\n",
    "])\n",
    "\n",
    "test_ds_downstream = ImageFolder(\n",
    "    root=\"new_test_w_nst_images_removed\",\n",
    "    transform=custom_transform\n",
    ")\n",
    "\n",
    "nu_classes = len(test_ds_downstream.classes)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Building the data loader\n",
    "test_dl_downstream = torch.utils.data.DataLoader(\n",
    "    test_ds_downstream,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=os.cpu_count(),\n",
    "    drop_last=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a8fe5-c20e-4776-9a46-77b13ebcf246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_true = 0\n",
    "# predicted_true = 0\n",
    "# correct_true = 0\n",
    "\n",
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "\n",
    "eval_model.eval()\n",
    "for i, element in enumerate(test_dl_downstream):\n",
    "    image, label = element\n",
    "    image = torch.Tensor(image).to(DEVICE)\n",
    "    # label = label.to(DEVICE)\n",
    "    y_pred = np.argmax(np.array(eval_model(image).detach().cpu()), axis=1)\n",
    "    \n",
    "    y_pred_list.extend(y_pred)\n",
    "    y_true_list.extend(label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # predicted_classes = torch.argmax(y_pred, dim=1) == 0\n",
    "    # target_classes = label\n",
    "    # target_true += torch.sum(target_classes == 0).float()\n",
    "    # predicted_true += torch.sum(predicted_classes).float()\n",
    "    # correct_true += torch.sum(\n",
    "    #     predicted_classes == target_classes & predicted_classes == 0).float()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e8bfcc-d3fb-4d11-8ef3-927a71940381",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(v == 1 for v in y_true_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7162b-d5d1-45e9-b038-6f41a6a69d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i, v in enumerate(y_pred_list):\n",
    "    if v == y_true_list[i]:\n",
    "        c+=1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e04cd-e1b4-4a19-8855-92da84554a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count = 0\n",
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "\n",
    "for i, v in enumerate(y_pred_list):\n",
    "    if v == y_true_list[i]:\n",
    "        correct_count+=1\n",
    "    if v == 1 and y_true_list[i] == 1:\n",
    "        tp+=1\n",
    "    if v == 0 and y_true_list[i] == 0:\n",
    "        tn+=1\n",
    "    if v == 1 and y_true_list[i] == 0:\n",
    "        fp+=1\n",
    "    if v == 0 and y_true_list[i] == 1:\n",
    "        fn+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e392e07-f35a-4bca-9584-6b84db8f7210",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtp\u001b[49m, tn, fp, fn\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tp' is not defined"
     ]
    }
   ],
   "source": [
    "tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c1ee9bca-5e2a-4dd7-9f2d-a7a3a7784fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = tp*100/(tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cff66f16-0cbb-469e-9d76-bf4f0a64cdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7063758389261745"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_count/(tp + tn + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e29846fb-e19c-48e6-be6b-217e8a584872",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tp*100/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e50be99d-894e-4c07-ab70-880449930edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.524663677130047"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d2b8cbb2-31e9-4f4d-bf6c-010e75735255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.894039735099337, 33.333333333333336)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06563e40-5317-4832-82b9-ae50879578c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
